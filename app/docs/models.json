[
    {
      "title": "ChatGPT",
      "install": "pip install openai or npm install openai",
      "pythonCode": "import openai\n\n# Replace with your OpenAI API key\napi_key = \"your-api-key\"\n\ndef get_ai_response(prompt):\n    try:\n        response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            api_key=api_key  # Pass the API key directly\n        )\n        return response[\"choices\"][0][\"message\"][\"content\"]\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage\nuser_input = input(\"Enter your prompt: \")\nai_response = get_ai_response(user_input)\nprint(\"AI Response:\", ai_response)",
      "nodejsCode": "const { OpenAI } = require(\"openai\");\n\n// Replace with your OpenAI API key\nconst openai = new OpenAI({\n    apiKey: \"your-api-key\",  // Ensure you keep this secret\n});\n\nasync function getAIResponse(prompt) {\n    try {\n        const response = await openai.chat.completions.create({\n            model: \"gpt-3.5-turbo\",\n            messages: [{ role: \"user\", content: prompt }],\n        });\n\n        console.log(\"AI Response:\", response.choices[0].message.content);\n    } catch (error) {\n        console.error(\"Error:\", error);\n    }\n}\n\nconst userInput = \"Hello, how can AI help me today?\";\ngetAIResponse(userInput);",
      "apiKeyUrl": "https://openai.com/api/"
    },
    {
      "title": "Gemini",
      "install": "pip install google-generativeai or npm install @google/generative-ai",
      "pythonCode": "import google.generativeai as genai\n\n# Replace with your API key\ngenai.configure(api_key=\"your-api-key\")\n\ndef get_gemini_response(prompt):\n    try:\n        model = genai.GenerativeModel(\"gemini-pro\")\n        response = model.generate_content(prompt)\n        return response.text\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage\nuser_input = input(\"Enter your prompt: \")\nai_response = get_gemini_response(user_input)\nprint(\"AI Response:\", ai_response)",
      "nodejsCode": "const { GoogleGenerativeAI } = require(\"@google/generative-ai\");\n\n// Replace with your API key\nconst genAI = new GoogleGenerativeAI(\"your-api-key\");\n\nasync function getGeminiResponse(prompt) {\n    try {\n        const model = genAI.getGenerativeModel({ model: \"gemini-pro\" });\n        const result = await model.generateContent(prompt);\n        console.log(\"AI Response:\", result.response.text);\n    } catch (error) {\n        console.error(\"Error:\", error);\n    }\n}\n\nconst userInput = \"Hello, how can AI help me today?\";\ngetGeminiResponse(userInput);",
      "apiKeyUrl": "https://console.cloud.google.com/apis/credentials"
    },
    {
      "title": "Ollama",
      "install": "pip install @langchain/ollama or npm install @langchain/ollama",
      "pythonCode": "from flask import Flask, request, jsonify\nfrom langchain.ollama import Ollama\n\n# Initialize the Ollama model\nllm = Ollama(\n    model=\"qwen2.5-coder:1.5b\",\n    temperature=0,\n    max_retries=2,\n)\n\napp = Flask(__name__)\n\n@app.route('/api/query', methods=['POST'])\ndef query():\n    try:\n        data = request.get_json()\n        query = data.get('query')\n\n        if not query:\n            return jsonify({'error': 'Query is required.'}), 400\n\n        # Invoke the LLM\n        result = llm.invoke(query)\n\n        print('LLM Result:', result)\n\n        # Return the response as JSON\n        return jsonify({'data': result})\n    except Exception as err:\n        print('Error invoking :', err)\n        return jsonify({'error': 'Failed to fetch.'}), 500\n\nif __name__ == '__main__':\n    app.run(port=3000)",
      "nodejsCode": "const express = require('express');\nconst bodyParser = require('body-parser');\nconst { Ollama } = require('@langchain/ollama');\n\n// Initialize the Ollama model\nconst llm = new Ollama({\n  model: 'qwen2.5-coder:1.5b',\n  temperature: 0,\n  maxRetries: 2,\n});\n\nconst app = express();\napp.use(bodyParser.json());\n\napp.post('/api/query', async (req, res) => {\n  try {\n    const { query } = req.body;\n\n    if (!query) {\n      return res.status(400).json({ error: 'Query is required .' });\n    }\n\n    // Invoke the LLM\n    const result = await llm.invoke(query);\n    \n    // Return the response as JSON\n    res.json({ data: result });\n  } catch (err) {\n    console.error('Error invoking Ollama:', err);\n    res.status(500).json({ error: 'Failed to fetch.' });\n  }\n});\n\nconst PORT = process.env.PORT || 3000;\napp.listen(PORT, () => {\n  console.log(`Server is running on port ${PORT}`);\n});",
      "apiKeyUrl": "https://console.cloud.google.com/apis/credentials"
    }
  ]
  